

# 量化对象

- **权重**（Weight）：weight的量化是最常规也是最常见的。量化weight可达到减少模型大小内存和占用空间。
- **激活**（Activation）：实际中activation往往是占内存使用的大头，因此量化activation不仅可以大大减少内存占用。更重要的是，结合weight的量化可以充分利用整数计算获得性能提升。
- **KV缓存**（KV Cache）：量化 KV 缓存对于提高长序列生成的吞吐量至关重要。
- 梯度（Gradients）：相对上面的量化对象略微小众一些，因为主要用于训练。在训练深度学习模型时，梯度通常是浮点数，它主要作用是在分布式计算中减少通信开销，同时，也可以减少backward时的开销。

# 量化分类

## 根据量化时机分类

根据应用量化压缩模型的阶段，可以将模型量化分为：

- **量化感知训练**（Quantization Aware Training，QAT）：在模型训练过程中加入伪量化算子，通过训练时统计输入输出的数据范围可以提升量化后模型的精度，适用于对模型精度要求较高的场景；其量化目标无缝地集成到模型的训练过程中。这种方法使LLM在训练过程中适应低精度表示，增强其处理由量化引起的精度损失的能力。这种适应旨在量化过程之后保持更高性能。
- **量化感知微调**（Quantization-Aware Fine-tuning，QAF）：在微调过程中对LLM进行量化。主要目标是确保经过微调的LLM在量化为较低位宽后仍保持性能。通过将量化感知整合到微调中，以在模型压缩和保持性能之间取得平衡。
- **训练后量化**（Post Training Quantization，PTQ）：在LLM训练完成后对其参数进行量化，只需要少量校准数据，适用于追求高易用性和缺乏训练资源的场景。主要目标是减少LLM的存储和计算复杂性，而无需对LLM架构进行修改或进行重新训练。PTQ的主要优势在于其简单性和高效性。但PTQ可能会在量化过程中引入一定程度的精度损失。

## 根据量化粒度分类



## 根据量化节点分类

根据量化节点的分布，可以分为**均匀量化和非均匀量化**。

# PTQ训练后量化方法

目前针对 LLM 的量化研究都集中在 Post-training quantization (PTQ) ——训练后量化。

| 量化方法    | 量化对象   |       |
| ----------- | ---------- | ----- |
| LLM.int8()  | 权重       |       |
| GPTQ        | 权重       |       |
| AWQ         | 权重       | W4A16 |
| SmoothQuant | 权重和激活 | W8A8  |
| INT8/FP8    | KV Cache   |       |

