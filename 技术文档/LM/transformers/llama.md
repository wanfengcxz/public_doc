









llamaçš„ä¸‹æ¸¸ä»»åŠ¡å°±æ˜¯å°†åŸºç¡€æ¨¡å‹è¾“å‡ºçš„hidden_statesæ”¾å…¥ä¸€ä¸ªç›¸ä¿¡å±‚ï¼ŒåŒæ—¶ä½¿ç”¨äº†ä¸åŒçš„æŸå¤±å‡½æ•°ã€‚





# LLaMA1

LLaMA1çš„å‚æ•°é‡

![image-20240626194559822](llama.assets/image-20240626194559822.png)



LLaMAå’ŒTransformerçš„å¯¹æ¯”

pre-RMSNorm

Qå’ŒKçš„RoPE

KVCache

SwiGLU

![image-20240626194109709](llama.assets/image-20240626194109709.png)



## RMSNorm

ç®€åŒ–äº†è®¡ç®—ï¼Œä½¿å¾—è®­ç»ƒ/æ¨ç†æ›´å¿«ã€‚

![image-20240626200218307](llama.assets/image-20240626200218307.png)

## PE

![image-20240626201352604](llama.assets/image-20240626201352604.png)

RoPE

![image-20240626201807292](llama.assets/image-20240626201807292.png)

![image-20240626201819309](llama.assets/image-20240626201819309.png)

# LLaMA2

LLaMA2ç›¸æ¯”äº1

- æ‰©å¤§äº†æ¨¡å‹ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œå³ä¸€æ¬¡èƒ½å¤Ÿè¾“å…¥çš„tokenä¸ªæ•°ã€‚
- å¯¹äº34Bå’Œ70Bæ¨¡å‹åŠ å…¥Grouped-Query Attention
- åŠ å…¥æ›´å¤šçš„è®­ç»ƒæ•°æ®ï¼ˆé‡è¦ï¼‰

![image-20240626194943086](llama.assets/image-20240626194943086.png)

## KV-Cache

åœ¨æ¨ç†æ—¶ï¼Œä¼šæœ‰é‡å¤è®¡ç®—ã€‚T=4æ—¶ï¼Œè®¡ç®—token1ï¼Œ2ï¼Œ3ï¼Œ4ä¹‹é—´çš„åˆ†æ•°ï¼Œä½†æ˜¯token1ï¼Œ2ï¼Œ3ä¹‹é—´çš„åˆ†æ•°åœ¨T=3æ—¶å·²ç»è®¡ç®—è¿‡äº†ã€‚

![image-20240627125133365](llama.assets/image-20240627125133365.png)

å› æ­¤T=4æ—¶ï¼Œåªéœ€è¦è®¡ç®—token4å’Œtoken1ï¼Œ2ï¼Œ3ï¼Œ4çš„åˆ†æ•°å°±å¯ä»¥äº†ã€‚

![image-20240627124448529](llama.assets/image-20240627124448529.png)

è¯¦ç»†ä¸€ç‚¹ï¼ŒT=1åˆ°T=4çš„æµç¨‹å¦‚ä¸‹ï¼Œå…¶ä¸­è“è‰²è¡¨ç¤ºKV Cacheç¼“å­˜çš„å€¼

![image-20240627195255178](llama.assets/image-20240627195255178.png)

é‡‡ç”¨äº† KV Cache çš„è¯ LLM çš„æ¨ç†è¿‡ç¨‹å¯ä»¥çœ‹æˆ **2 ä¸ªé˜¶æ®µ**

1. ç¬¬ä¸€æ¬¡è¿­ä»£çš„æ—¶å€™ï¼Œæ­¤æ—¶ KV Cache ä¸ºç©ºï¼Œæ‰€æœ‰çš„è¾“å…¥çš„ token éƒ½éœ€è¦ä¸ºå…¶è®¡ç®— key, value, query å‘é‡ï¼Œå…¶ä¸­ key å’Œ value ä¼šè¢«ç¼“å­˜èµ·æ¥
2. åç»­çš„æ¯ä¸€æ¬¡è¿­ä»£ï¼Œ**åªéœ€è¦ä¸ºæ–°çš„ token è®¡ç®— keyã€valueã€query**ï¼Œå¹¶æ›´æ–° KV Cache

KV Cache åŠ é€Ÿæ¨ç†çš„ä»£ä»·æ˜¯æ˜¾å­˜å ç”¨ä¼šå˜é«˜ï¼Œæ‰€ä»¥å®ƒæ˜¯**ç©ºé—´æ¢æ—¶é—´**çš„åŠæ³•ã€‚å…³äºå¼€ä¸å¼€ KV Cache çš„æ˜¾å­˜å ç”¨çš„å¯¹æ¯”å¯ä»¥æ€»ç»“ä¸ºï¼š

- ç”¨ KV Cache - `2 * hidden_size * num_layers * decoder_length`
- ä¸ç”¨ KV Cache - `2 * hidden_size * 1 * decoder_length`



[LLM æ¨ç†åŠ é€Ÿ - KV Cache - MartinLwx's Blog](https://martinlwx.github.io/zh-cn/llm-inference-optimization-kv-cache/)

[Generate: using k-v cache is faster but no difference to memory usage - ğŸ¤—Transformers - Hugging Face Forums](https://discuss.huggingface.co/t/generate-using-k-v-cache-is-faster-but-no-difference-to-memory-usage/31272)

## Grouped Multi-Query Attention

Fast Transformer Decoding: One Write-Head is All You Need

Qè¿˜æ˜¯å¤šä¸ªheadï¼Œä½†æ˜¯Kå’ŒVå˜æˆä¸€ä¸ªheadã€‚

```bash
# åŸå§‹
Q ()

# å½“å‰
```

