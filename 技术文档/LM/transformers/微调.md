

# 有监督微调

- 使用 **人工标注的高质量对话数据**（例如用户提问+理想回答的配对）。

- 数据示例：

  ```
用户：如何做番茄炒蛋？
  助手：1. 准备食材：番茄2个，鸡蛋3个... 2. 热锅倒油...
  ```
- 在基座模型上继续训练，最小化生成回答与标注答案的差异（交叉熵损失）。

## SFT中模型输出的答案长度与标准答案不一致时（

！！！！不会出现长度不一致，此时是训练模式，不是推理，输入N个token那么输出就是N个token。

在监督微调（SFT）中，当模型输出的答案长度与标准答案不一致时，核心解决方法是 **通过掩码（Mask）忽略无效位置（如填充部分或非目标位置）的损失**。具体流程如下：

1. **输入构造**：将输入prompt和目标答案拼接，得到完整序列
2. **目标序列**：右移序列一位，得到预测目标
3. **掩码设计**：仅对答案部分的 token 计算损失，忽略 prompt 和填充部分的损失。

举个例子，假设：

- Prompt: "What is AI?" → 分词为 `[10, 20, 30]`
- 答案: "AI is Artificial Intelligence." → 分词为 `[40, 50, 60, 70, 4]`（`4` 为 `<EOS>`）

**输入序列与目标序列**：
| 输入序列（模型输入） | `[10, 20, 30, 40, 50, 60, 70]` |
| 目标序列（右移一位） | `[20, 30, 40, 50, 60, 70, 4]` |

**掩码（精巧的设计）**：

- 仅计算答案部分（从 `40` 开始）的损失：
  `[0, 0, 1, 1, 1, 1, 1]`（`0` 表示忽略，`1` 表示计算损失）。

# LoRA

https://www.zhihu.com/tardis/zm/art/623543497?source_id=1005