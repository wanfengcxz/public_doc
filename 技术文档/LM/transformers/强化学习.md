

**强化学习（Reinforcement Learning, RL）**，通过「奖励和惩罚」机制，教会模型生成更符合预期的结果。

**RLHF，通过人类反馈（直接或间接）告诉模型“什么是对的”，让模型自我优化**。

# **RLHF（基于人类反馈的强化学习）**

步骤拆解（以聊天机器人为例）：

1. **监督微调（SFT）**
   - 先用人工标注的高质量问答数据微调模型，让它初步学会「正确回答」。
2. **训练奖励模型（Reward Model）**
   - 让模型对同一个问题生成多个答案（比如生硬回答、友好回答、有害回答）。
   - 让人类标注员对这些答案排序（比如：友好回答 > 生硬回答 > 有害回答）。
   - 训练一个「奖励模型」，学习人类偏好，给答案打分（例如友好回答得高分，有害回答扣分）。
3. **强化学习优化（如PPO算法）**
   - 模型生成答案 → 奖励模型打分 → 模型根据得分调整参数 → 下次生成更高分的答案。
   - 反复迭代，模型逐渐学会输出更友好、安全的回答。

# **DPO（直接偏好优化）**

#### 特点：

- 不需要单独训练奖励模型，直接用人类标注的偏好数据调整模型。
- 原理：对比「好的回答」和「差的回答」，让模型参数直接向「好回答」对齐。

#### 🌰 例子：

- 问题：“推荐一道健康的菜品？”
  - 差回答：“炸蛋”（标注员标记为差）
  - 好回答：“清蒸鱼配西兰花”（标注员标记为好）
- 模型通过DPO直接学习到「清蒸鱼」比「炸蛋」更健康，逐渐调整参数。