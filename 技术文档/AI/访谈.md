**腾讯新闻：从这几位颇有名望的导师身上学到了什么？**

**杨植麟：**我学习到最多是在Google，实习了很长时间。2018年底开始做基于Transformer的语言模型，最大learning是从无限雕花中把自己释放出来，这很关键。

应该看什么是大方向、大梯度。当你眼前有十条路，一般人考虑我走这条路前面有一个行人怎么刹车，是短期细节，但这十条路到底选哪一条最重要。

这个领域在之前有这样的问题。比如，在只有一两百万token（标记）的数据集上，看perplexity（困惑度，衡量模型在预测序列时的不确定性或混乱度）怎么降得更低，loss（损失，模型在训练过程中的误差或损失函数的值）怎么降得更低，怎么提升准确率，你会陷入无限雕花。有人发明很多诡异的architecture（架构），这些是雕花技巧。雕花之后可能在这种数据集上变好，但没看到问题本质。

本质在于，要去分析这个领域缺少的是什么？第一性原理是什么？

Scaling law为什么能成为第一性原理？你只要能找到一个结构，满足两个条件：一是足够通用，二是可规模化。通用是你把所有问题放到这个框架建模，可规模化是只要你投入足够多算力，它就能变好。

这是我在Google学到的思维：如果能被更底层的东西解释，就不应该在上层过度雕花。有一句重要的话我很认同：如果你能用scale解决的问题，就不要用新的算法解决。新算法最大价值是让它怎么更好的scale。当你把自己从雕花的事中释放出来，可以看到更多。